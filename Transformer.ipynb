{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "56587a03cdf74cb9b2b5564156a00f5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_788fcacece3a4262bacc669f42b4c2bc",
              "IPY_MODEL_84b4480ba6d2434396b4ff5de9a0e8a4",
              "IPY_MODEL_28f9406d4d764c76a457f41c09014691"
            ],
            "layout": "IPY_MODEL_343caa85af98468f9928c7619f71cb3a"
          }
        },
        "788fcacece3a4262bacc669f42b4c2bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4a8f74cdd2a4191960b0b1dfbe763e9",
            "placeholder": "​",
            "style": "IPY_MODEL_a2deb1a770c5435981840e0e0324d292",
            "value": "Map: 100%"
          }
        },
        "84b4480ba6d2434396b4ff5de9a0e8a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa4e17cebebf4a198872fee8444f47a7",
            "max": 36718,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a38d5de9fee548f3a648bd5c4b9b49ca",
            "value": 36718
          }
        },
        "28f9406d4d764c76a457f41c09014691": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ffba5fb391c4c34a7a132a9a2e38283",
            "placeholder": "​",
            "style": "IPY_MODEL_68aa78279e81455ea2b65531b464cc60",
            "value": " 36718/36718 [00:07&lt;00:00, 3615.26 examples/s]"
          }
        },
        "343caa85af98468f9928c7619f71cb3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4a8f74cdd2a4191960b0b1dfbe763e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2deb1a770c5435981840e0e0324d292": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa4e17cebebf4a198872fee8444f47a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a38d5de9fee548f3a648bd5c4b9b49ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7ffba5fb391c4c34a7a132a9a2e38283": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68aa78279e81455ea2b65531b464cc60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from typing import Optional, Tuple, List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "_1zhp-q1FAmu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "id": "DQdSUcOsEf6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5862f356-7d3d-4c9a-93fc-e81c72d6a17b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x + self.pe[:, : x.size(1)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class EmbeddingsWithPositionalEncoding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model, max_len=max_len, dropout=dropout)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
        "        # token_ids: (B, T)\n",
        "        x = self.token_embedding(token_ids) * math.sqrt(self.d_model)\n",
        "        x = self.pos_enc(x)\n",
        "        return x  # (B, T, d_model)"
      ],
      "metadata": {
        "id": "qoqXxidDINrE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, attn_dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(attn_dropout) if attn_dropout > 0 else nn.Identity()\n",
        "\n",
        "    def forward(self,\n",
        "                Q: torch.Tensor,\n",
        "                K: torch.Tensor,\n",
        "                V: torch.Tensor,\n",
        "                mask: Optional[torch.Tensor] = None,\n",
        "                return_attn: bool = False) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
        "        # Q,K,V: (B, H, T_q, d_k), (B, H, T_k, d_k), (B, H, T_k, d_k)\n",
        "        d_k = Q.size(-1)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)  # (B, H, T_q, T_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            # mask: broadcastable to (B, 1, T_q, T_k) or (B, H, T_q, T_k)\n",
        "            if mask.dtype != torch.bool:\n",
        "                mask = mask.to(torch.bool)\n",
        "            scores = scores.masked_fill(~mask, -1e9)\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "        out = torch.matmul(attn, V)  # (B, H, T_q, d_k)\n",
        "        return (out, attn) if return_attn else (out, None)"
      ],
      "metadata": {
        "id": "ax2BWcsAJOJL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, num_heads: int, attn_dropout: float = 0.0, proj_dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.in_proj = nn.Linear(d_model, 3 * d_model)\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "        self.attn_fn = ScaledDotProductAttention(attn_dropout)\n",
        "        self.proj_dropout = nn.Dropout(proj_dropout) if proj_dropout > 0 else nn.Identity()\n",
        "\n",
        "    def _split_heads(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: (B, T, d_model) -> (B, H, T, d_k)\n",
        "        B, T, _ = x.shape\n",
        "        return x.view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def _combine_heads(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: (B, H, T, d_k) -> (B, T, d_model)\n",
        "        B, H, T, d_k = x.shape\n",
        "        return x.transpose(1, 2).contiguous().view(B, T, H * d_k)\n",
        "\n",
        "    def forward(self, x_q: torch.Tensor, x_kv: Optional[torch.Tensor] = None,\n",
        "                mask: Optional[torch.Tensor] = None, return_attn: bool = False):\n",
        "        # x_q: (B, T_q, d_model); x_kv: (B, T_k, d_model) or None\n",
        "        if x_kv is None:\n",
        "            x_kv = x_q\n",
        "\n",
        "        if x_q is x_kv:\n",
        "            qkv = self.in_proj(x_q)  # (B, T, 3*d_model)\n",
        "            q, k, v = qkv.split(self.d_model, dim=-1)\n",
        "        else:\n",
        "            q = self.in_proj(x_q)[:, :, :self.d_model]\n",
        "            kv = self.in_proj(x_kv)[:, :, self.d_model:]\n",
        "            k, v = kv.split(self.d_model, dim=-1)\n",
        "\n",
        "        Q = self._split_heads(q)  # (B, H, T_q, d_k)\n",
        "        K = self._split_heads(k)  # (B, H, T_k, d_k)\n",
        "        V = self._split_heads(v)  # (B, H, T_k, d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            if mask.dtype != torch.bool:\n",
        "                mask = mask.to(torch.bool)\n",
        "            mask = mask.to(Q.device)\n",
        "\n",
        "        attn_out, attn_weights = self.attn_fn(Q, K, V, mask=mask, return_attn=return_attn)  # (B, H, T_q, d_k)\n",
        "        combined = self._combine_heads(attn_out)  # (B, T_q, d_model)\n",
        "        out = self.out_proj(combined)\n",
        "        out = self.proj_dropout(out)\n",
        "        return (out, attn_weights) if return_attn else (out, None)\n"
      ],
      "metadata": {
        "id": "AyckVRhxJn88"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_padding_mask(pad_mask: torch.Tensor) -> torch.Tensor:\n",
        "    if pad_mask.dtype != torch.bool:\n",
        "        pad_mask = pad_mask != 0\n",
        "    return pad_mask.unsqueeze(1).unsqueeze(2)  # (B,1,1,T)\n",
        "\n",
        "def make_causal_mask(seq_len: int, device: torch.device) -> torch.Tensor:\n",
        "    causal = torch.tril(torch.ones((seq_len, seq_len), dtype=torch.bool, device=device))\n",
        "    return causal.unsqueeze(0).unsqueeze(0)  # (1,1,T,T)\n",
        "\n",
        "def combine_masks(*masks: Optional[torch.Tensor]) -> Optional[torch.Tensor]:\n",
        "    masks = [m for m in masks if m is not None]\n",
        "    if not masks:\n",
        "        return None\n",
        "    out = masks[0]\n",
        "    for m in masks[1:]:\n",
        "        out = out & m\n",
        "    return out"
      ],
      "metadata": {
        "id": "J5TveDPzJ4-z"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads, attn_dropout=dropout, proj_dropout=dropout)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.drop1 = nn.Dropout(dropout)\n",
        "        self.drop2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, src_mask: Optional[torch.Tensor] = None):\n",
        "        attn_out, _ = self.self_attn(x, x, mask=src_mask)\n",
        "        x = x + self.drop1(attn_out)\n",
        "        x = self.norm1(x)\n",
        "        ffn_out = self.ffn(x)\n",
        "        x = x + self.drop2(ffn_out)\n",
        "        x = self.norm2(x)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size: int, d_model: int, n_layers: int, n_heads: int, d_ff: int, max_len: int = 5000, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.embed = EmbeddingsWithPositionalEncoding(vocab_size, d_model, max_len=max_len, dropout=dropout)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
        "        self.final_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, src_ids: torch.Tensor, src_mask: Optional[torch.Tensor] = None):\n",
        "        x = self.embed(src_ids)  # (B, T, d_model)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, src_mask)\n",
        "        x = self.final_norm(x)\n",
        "        return x  # (B, T, d_model)"
      ],
      "metadata": {
        "id": "xN23qVwRKO7L"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads, attn_dropout=dropout, proj_dropout=dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads, attn_dropout=dropout, proj_dropout=dropout)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.drop1 = nn.Dropout(dropout)\n",
        "        self.drop2 = nn.Dropout(dropout)\n",
        "        self.drop3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, enc_output: torch.Tensor,\n",
        "                tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None):\n",
        "        self_attn_out, _ = self.self_attn(x, x, mask=tgt_mask)\n",
        "        x = x + self.drop1(self_attn_out)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        cross_attn_out, _ = self.cross_attn(x, enc_output, mask=memory_mask)\n",
        "        x = x + self.drop2(cross_attn_out)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        ffn_out = self.ffn(x)\n",
        "        x = x + self.drop3(ffn_out)\n",
        "        x = self.norm3(x)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size: int, d_model: int, n_layers: int, n_heads: int, d_ff: int, max_len: int = 5000, dropout: float = 0.3, tie_embeddings: bool = False):\n",
        "        super().__init__()\n",
        "        self.embed = EmbeddingsWithPositionalEncoding(vocab_size, d_model, max_len=max_len, dropout=dropout)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
        "        self.final_norm = nn.LayerNorm(d_model)\n",
        "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
        "        if tie_embeddings:\n",
        "            self.output_proj.weight = self.embed.token_embedding.weight\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, tgt_ids: torch.Tensor, enc_output: torch.Tensor,\n",
        "                tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None):\n",
        "        x = self.embed(tgt_ids)  # (B,T,d_model)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_output, tgt_mask=tgt_mask, memory_mask=memory_mask)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.output_proj(x)  # (B,T,vocab)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "4doAAhAfKowd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self, src_vocab: int, tgt_vocab: int, src_pad_idx: int, tgt_pad_idx: int,\n",
        "                 d_model: int = 512, n_layers: int = 6, n_heads: int = 8, d_ff: int = 2048, max_len: int = 5000, dropout: float = 0.1, tie_embeddings: bool = False):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, d_model, n_layers, n_heads, d_ff, max_len=max_len, dropout=dropout)\n",
        "        self.decoder = Decoder(tgt_vocab, d_model, n_layers, n_heads, d_ff, max_len=max_len, dropout=dropout, tie_embeddings=tie_embeddings)\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.tgt_pad_idx = tgt_pad_idx\n",
        "\n",
        "    def make_src_mask(self, src_ids: torch.Tensor) -> torch.Tensor:\n",
        "        # src_ids: (B, S)\n",
        "        return (src_ids != self.src_pad_idx).unsqueeze(1).unsqueeze(2)  # (B,1,1,S)\n",
        "\n",
        "    def make_tgt_mask(self, tgt_ids: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size, tgt_len = tgt_ids.size()\n",
        "        pad_mask = (tgt_ids != self.tgt_pad_idx).unsqueeze(1).unsqueeze(3)  # (B,1,T,1)\n",
        "        causal = make_causal_mask(tgt_len, device=tgt_ids.device)  # (1,1,T,T)\n",
        "        return pad_mask & causal  # (B,1,T,T)\n",
        "\n",
        "    def forward(self, src_ids: torch.Tensor, tgt_ids: torch.Tensor):\n",
        "        src_mask = self.make_src_mask(src_ids)  # (B,1,1,S)\n",
        "        tgt_mask = self.make_tgt_mask(tgt_ids)  # (B,1,T,T)\n",
        "        memory_mask = src_mask\n",
        "\n",
        "        enc_output = self.encoder(src_ids, src_mask)\n",
        "        logits = self.decoder(tgt_ids, enc_output, tgt_mask=tgt_mask, memory_mask=memory_mask)  # (B,T,V)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "eGJ5ZwVpLJfP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n",
        "\n",
        "PAD_ID = tokenizer.pad_token_id\n",
        "VOCAB_SIZE = len(tokenizer)\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def tokenize_batch(examples):\n",
        "    out = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
        "    return {\"input_ids\": out[\"input_ids\"]}\n",
        "\n",
        "tokenized = dataset.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "tokenized.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
        "\n",
        "train_loader = DataLoader(tokenized[\"train\"], batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader   = DataLoader(tokenized[\"validation\"], batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "jrtXPzjTMTai",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "56587a03cdf74cb9b2b5564156a00f5c",
            "788fcacece3a4262bacc669f42b4c2bc",
            "84b4480ba6d2434396b4ff5de9a0e8a4",
            "28f9406d4d764c76a457f41c09014691",
            "343caa85af98468f9928c7619f71cb3a",
            "e4a8f74cdd2a4191960b0b1dfbe763e9",
            "a2deb1a770c5435981840e0e0324d292",
            "aa4e17cebebf4a198872fee8444f47a7",
            "a38d5de9fee548f3a648bd5c4b9b49ca",
            "7ffba5fb391c4c34a7a132a9a2e38283",
            "68aa78279e81455ea2b65531b464cc60"
          ]
        },
        "outputId": "a897e090-ed52-46cb-c352-de41c5d7a52d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/36718 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56587a03cdf74cb9b2b5564156a00f5c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model: nn.Module, data_loader: DataLoader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            # batch is a dict-like row from datasets with \"input_ids\" tensor shaped (B, L)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            tgt_input = input_ids[:, :-1]\n",
        "            tgt_output = input_ids[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            logits = model(input_ids, tgt_input)  # (B, T-1, V)\n",
        "            logits = logits.view(-1, logits.size(-1))\n",
        "            loss = criterion(logits, tgt_output)  # summed loss over non-ignored tokens\n",
        "\n",
        "            non_pad = (tgt_output != PAD_ID).sum().item()\n",
        "            if non_pad == 0:\n",
        "                continue\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_tokens += non_pad\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    return avg_loss, math.exp(avg_loss)\n",
        "\n",
        "\n",
        "def train_loop(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader,\n",
        "               epochs: int = 6, lr: float = 3e-4, device: torch.device = device, prev_val: float = float('inf'), count: int = 0):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID, label_smoothing=0.1, reduction='sum')  # sum so we can divide by token count\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_tokens = 0\n",
        "        for step, batch in enumerate(train_loader, start=1):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            tgt_input = input_ids[:, :-1]\n",
        "            tgt_output = input_ids[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(input_ids, tgt_input)  # (B, T-1, V)\n",
        "            logits_flat = logits.view(-1, logits.size(-1))\n",
        "            loss = criterion(logits_flat, tgt_output)  # summed loss\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            non_pad = (tgt_output != PAD_ID).sum().item()\n",
        "            if non_pad > 0:\n",
        "                running_loss += loss.item()\n",
        "                running_tokens += non_pad\n",
        "\n",
        "            if step % 200 == 0:\n",
        "                cur_avg = (running_loss / running_tokens) if running_tokens > 0 else float(\"nan\")\n",
        "                print(f\"Epoch {epoch} Step {step} | partial train_loss_tokavg={cur_avg:.6f}\")\n",
        "\n",
        "        train_loss = running_loss / running_tokens\n",
        "        train_ppl = math.exp(train_loss)\n",
        "\n",
        "        val_loss, val_ppl = evaluate(model, val_loader, criterion, device)\n",
        "        print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} PPL: {train_ppl:.2f} | Val Loss: {val_loss:.4f} PPL: {val_ppl:.2f}\")\n",
        "        if prev_val > val_loss: prev_val = val_loss\n",
        "        elif prev_val == val_loss:\n",
        "          count += 1\n",
        "          if count > 2: print(\"same val_loss repeated for 3 epochs!!!\"); break\n",
        "        else: print(\"Overfitting!!!\"); break\n"
      ],
      "metadata": {
        "id": "uTqelokrMYoM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D_MODEL = 512\n",
        "N_LAYERS = 4\n",
        "N_HEADS = 8\n",
        "D_FF = 2048\n",
        "\n",
        "model = Seq2SeqTransformer(\n",
        "    src_vocab=VOCAB_SIZE,\n",
        "    tgt_vocab=VOCAB_SIZE,\n",
        "    src_pad_idx=PAD_ID,\n",
        "    tgt_pad_idx=PAD_ID,\n",
        "    d_model=D_MODEL,\n",
        "    n_layers=N_LAYERS,\n",
        "    n_heads=N_HEADS,\n",
        "    d_ff=D_FF,\n",
        "    max_len=MAX_LEN,\n",
        "    dropout=0.1,\n",
        "    tie_embeddings=False\n",
        ").to(device)\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "input_ids = batch[\"input_ids\"]\n",
        "print(\"Smoke batch shapes:\", input_ids.shape)\n",
        "\n",
        "train_loop(model, train_loader, val_loader, epochs=10, lr=3e-4, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfr8uXc4MkPT",
        "outputId": "fbd8a852-295b-4510-f67e-01aecf81e036"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Smoke batch shapes: torch.Size([32, 128])\n",
            "Epoch 1 Step 200 | partial train_loss_tokavg=7.688379\n",
            "Epoch 1 Step 400 | partial train_loss_tokavg=7.395458\n",
            "Epoch 1 Step 600 | partial train_loss_tokavg=7.205550\n",
            "Epoch 1 Step 800 | partial train_loss_tokavg=7.064069\n",
            "Epoch 1 Step 1000 | partial train_loss_tokavg=6.946462\n",
            "Epoch 01 | Train Loss: 6.8726 PPL: 965.44 | Val Loss: 6.4795 PPL: 651.67\n",
            "Epoch 2 Step 200 | partial train_loss_tokavg=6.079474\n",
            "Epoch 2 Step 400 | partial train_loss_tokavg=6.047121\n",
            "Epoch 2 Step 600 | partial train_loss_tokavg=6.005047\n",
            "Epoch 2 Step 800 | partial train_loss_tokavg=5.970900\n",
            "Epoch 2 Step 1000 | partial train_loss_tokavg=5.933210\n",
            "Epoch 02 | Train Loss: 5.9103 PPL: 368.83 | Val Loss: 6.0728 PPL: 433.91\n",
            "Epoch 3 Step 200 | partial train_loss_tokavg=5.402452\n",
            "Epoch 3 Step 400 | partial train_loss_tokavg=5.391234\n",
            "Epoch 3 Step 600 | partial train_loss_tokavg=5.389658\n",
            "Epoch 3 Step 800 | partial train_loss_tokavg=5.380993\n",
            "Epoch 3 Step 1000 | partial train_loss_tokavg=5.364440\n",
            "Epoch 03 | Train Loss: 5.3551 PPL: 211.69 | Val Loss: 5.8664 PPL: 352.99\n",
            "Epoch 4 Step 200 | partial train_loss_tokavg=4.935573\n",
            "Epoch 4 Step 400 | partial train_loss_tokavg=4.940511\n",
            "Epoch 4 Step 600 | partial train_loss_tokavg=4.938898\n",
            "Epoch 4 Step 800 | partial train_loss_tokavg=4.945960\n",
            "Epoch 4 Step 1000 | partial train_loss_tokavg=4.946374\n",
            "Epoch 04 | Train Loss: 4.9461 PPL: 140.62 | Val Loss: 5.7606 PPL: 317.53\n",
            "Epoch 5 Step 200 | partial train_loss_tokavg=4.566062\n",
            "Epoch 5 Step 400 | partial train_loss_tokavg=4.579818\n",
            "Epoch 5 Step 600 | partial train_loss_tokavg=4.591270\n",
            "Epoch 5 Step 800 | partial train_loss_tokavg=4.605575\n",
            "Epoch 5 Step 1000 | partial train_loss_tokavg=4.614454\n",
            "Epoch 05 | Train Loss: 4.6230 PPL: 101.79 | Val Loss: 5.7208 PPL: 305.14\n",
            "Epoch 6 Step 200 | partial train_loss_tokavg=4.265956\n",
            "Epoch 6 Step 400 | partial train_loss_tokavg=4.294357\n",
            "Epoch 6 Step 600 | partial train_loss_tokavg=4.310216\n",
            "Epoch 6 Step 800 | partial train_loss_tokavg=4.326807\n",
            "Epoch 6 Step 1000 | partial train_loss_tokavg=4.341431\n",
            "Epoch 06 | Train Loss: 4.3500 PPL: 77.48 | Val Loss: 5.7036 PPL: 299.94\n",
            "Epoch 7 Step 200 | partial train_loss_tokavg=4.020341\n",
            "Epoch 7 Step 400 | partial train_loss_tokavg=4.049141\n",
            "Epoch 7 Step 600 | partial train_loss_tokavg=4.072847\n",
            "Epoch 7 Step 800 | partial train_loss_tokavg=4.096740\n",
            "Epoch 7 Step 1000 | partial train_loss_tokavg=4.116130\n",
            "Epoch 07 | Train Loss: 4.1265 PPL: 61.96 | Val Loss: 5.7274 PPL: 307.17\n",
            "Overfitting!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate_text(model: nn.Module, tokenizer, prompt: str, max_len: int = 50, temperature: float = 2.0, top_k: int = 50, device: torch.device = device):\n",
        "    model.eval()\n",
        "\n",
        "    src_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "    enc_output = model.encoder(src_ids)\n",
        "    generated = torch.tensor([[tokenizer.pad_token_id]], device=device)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        tgt_mask = model.make_tgt_mask(generated)\n",
        "        memory_mask = model.make_src_mask(src_ids)\n",
        "\n",
        "        logits = model.decoder(generated, enc_output, tgt_mask=tgt_mask, memory_mask=memory_mask)\n",
        "        next_token_logits = logits[0, -1, :] / temperature\n",
        "\n",
        "        last_token = generated[0, -1]\n",
        "        next_token_logits[last_token] -= 1.0\n",
        "\n",
        "        top_vals, top_idx = torch.topk(next_token_logits, top_k)\n",
        "        probs = F.softmax(top_vals, dim=-1)\n",
        "        next_token = top_idx[torch.multinomial(probs, num_samples=1)]\n",
        "\n",
        "        generated = torch.cat([generated, next_token.unsqueeze(0)], dim=1)\n",
        "\n",
        "        if next_token.item() == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    output_text = tokenizer.decode(generated[0, 1:], skip_special_tokens=True)\n",
        "    return output_text"
      ],
      "metadata": {
        "id": "b76-cEug0jDC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"In the middle of the desert, a lone traveler found\"\n",
        "\n",
        "generated_text = generate_text(model, tokenizer, prompt, max_len=100, temperature=1.5, top_k=50, device=device)\n",
        "\n",
        "print(\"Generated text:\\n\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrCkFkrPdoSF",
        "outputId": "a0992ed7-f02b-4920-b4c7-7551e836344b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            "  Officer of a foundation of 150 million copies built a, representing determining a project in the case of the present the cause of bank of 150 tons of sustributaries and the allegedly the water a combined effort which witnessed by the border of a the basin found the,000 iron pl Such cause for the sector of the see the most accurate grant the famine of basin the use of the Crown Point Edward Coke Ridge basin below made bank of 500 . Like Cats the search engine the famine the most disassembly of the\n"
          ]
        }
      ]
    }
  ]
}